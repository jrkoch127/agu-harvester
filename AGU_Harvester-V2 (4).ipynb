{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79e8863",
   "metadata": {},
   "source": [
    "# AGU Harvester Notebook - Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e177c",
   "metadata": {},
   "source": [
    "Process: Version 2 is different in the way it treats the Role queries. When there is a big list of roles to query, the python can be much slower and fail to append each item to a DataFrame, so instead this version saves the data locally as json files and later will push the data to a DataFrame. This makes it easier to stop/start my queries at any point and pick up where I left off.\n",
    "\n",
    "First I query for paper metadata, put it into a DataFrame, then extract a list of the Role IDs for each author of the papers from the DataFrame. Then a new API query is run for each RoleID and exports the resulting metadata to a new json file saved locally in a 'json_roles' folder'. After all the roles are retrieved, I append the json data from each file to a DataFrame of affiliations.\n",
    "\n",
    "Finally, the abstracts and the affiliations are merged together into a single data frame, and I output the records to json and tagged format.\n",
    "\n",
    "API Note: The role queries may stall part way through, but can be stopped and continued as needed. About 100k roles may take about 3+ hours.\n",
    "\n",
    "----------\n",
    "\n",
    "Input:\n",
    "- Meeting code and Publication/meeting info\n",
    "- Local path to run this notebook\n",
    "\n",
    "Outfiles:\n",
    "- Excel files: AGU Abstracts (\"papers.xlsx\"), Affiliation/Roles (\"roles.xlsx\"), and final merged data (\"results.xlsx\")\n",
    "- AGU Author Affiliations/Roles as individual json files in directory \"json_roles\"\n",
    "- Json file and ADS Tagged file for ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f704e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys, os, io\n",
    "import argparse\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import re\n",
    "import unicodedata\n",
    "from pyingest.serializers.classic import Tagged\n",
    "\n",
    "# Inputs\n",
    "path = '/Users/sao/Documents/Python-Projects/AGU/version2/'\n",
    "meetingcode = \"fm22\"\n",
    "pub = \"AGU Fall Meeting 2022, held in Chicago, IL, 12-16 December 2022, id. \"\n",
    "pubdate = \"12/2022\"\n",
    "\n",
    "# Outputs\n",
    "json_output = meetingcode + \".json\"\n",
    "tagged = meetingcode + \".tag\"\n",
    "papers_outfile = meetingcode + \"_papers.xlsx\"\n",
    "roles_outfile = meetingcode + \"_roles.xlsx\"\n",
    "merged_outfile = meetingcode + \"_results.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56a9df",
   "metadata": {},
   "source": [
    "# Get AGU Meeting Abstracts, Author Roles w/ Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0612917",
   "metadata": {},
   
   "source": [
    "# ABSTRACTS API REQUEST\n",
    "# Check if papers file already exists, then proceed with gathering Role IDs\n",
    "# else: query the API for papers and then gather Role IDs\n",
    "\n",
    "os.chdir(path)\n",
    "if os.path.exists(papers_outfile):\n",
    "    paper_results = pd.read_excel(papers_outfile)\n",
    "    print('Read',len(paper_results),'papers from file')\n",
    "    \n",
    "    dt = paper_results.explode(\"ChildList_Role\")\n",
    "    roles = [eval(l) for l in dt['ChildList_Role'].to_list()]\n",
    "        # Extract role IDs from data, resulting in list of lists\n",
    "        # The function 'eval' does this: \n",
    "        # Transform\n",
    "        #  [\"['Role/4501570', 'Role/4503884']\",\"['Role/4191734', 'Role/4191863']\"] \n",
    "        # into\n",
    "        # [['Role/4501570', 'Role/4503884'],['Role/4191734', 'Role/4191863']]\n",
    "        # Initially the result is not a proper list of lists, but a list of strings\n",
    "        # that need to be turned into proper lists.\n",
    "        # Now we need to turn the list of lists into just one big list (\"flatten\" the list)\n",
    "    roles = [item for sublist in roles for item in sublist]\n",
    "    roles = list(set(roles))\n",
    "    print('Read',len(roles),'author roles from file')\n",
    "else:\n",
    "    # API Query for Paper data\n",
    "    domain = \"https://agu.confex.com/agu/meetingapi.cgi/Paper\"\n",
    "    AGU_API = domain[:27] + meetingcode + '/' + domain[27:]\n",
    "    data = requests.get(AGU_API).json()\n",
    "    print('Got',len(data),'papers from',meetingcode)\n",
    "\n",
    "    # Extract specified metadata from results\n",
    "    paper_results = pd.json_normalize(data)\n",
    "    paper_results = pd.DataFrame(paper_results)\n",
    "    paper_results = paper_results[[\n",
    "        \"Abstract\",\n",
    "        \"Title\",\n",
    "        \"ChildList_Role\",\n",
    "        \"FinalPaperNumber\",\n",
    "        \"_url\",\n",
    "        \"Withdrawn\",\n",
    "        \"GoodType\"\n",
    "    ]]\n",
    "\n",
    "    # Drop rows where GoodType = Break (these aren't abstracts); Drop withdrawn papers\n",
    "    paper_results = paper_results[paper_results.GoodType != 'Break']\n",
    "    paper_results = paper_results[paper_results.Withdrawn != 'w']\n",
    "    paper_results = paper_results.drop('Withdrawn',axis=1)\n",
    "    \n",
    "    # Save excel file of paper results\n",
    "    paper_results.to_excel(papers_outfile, sheet_name='paper_results', index=False)\n",
    "    print(\"Finalized and saved\",len(paper_results),\"papers to\",papers_outfile)\n",
    "\n",
    "    # Prepare roles list to query API for roles/affiliations\n",
    "    # Role IDs to role_list and deduplicate\n",
    "    dt = paper_results.explode(\"ChildList_Role\")\n",
    "    roles = dt['ChildList_Role'].to_list()\n",
    "    roles = list(set(roles))\n",
    "    roles = [item for item in roles if not(pd.isnull(item)) == True]\n",
    "    print('Extracted', len(roles),'author roles to query')\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# ROLES API REQUEST\n",
    "domain = \"https://agu.confex.com/agu/meetingapi.cgi/\"\n",
    "AGU_API = domain[:27] + meetingcode + '/' + domain[27:]\n",
    "\n",
    "# Check if roles folder exists, else make one\n",
    "if os.path.exists(\"json_roles\"):\n",
    "    os.chdir(\"json_roles\")\n",
    "else:\n",
    "    os.mkdir(\"json_roles\")\n",
    "    os.chdir(\"json_roles\")\n",
    "\n",
    "print('Started role requests at',datetime.datetime.now(),'...\\n') # Start clock on API queries\n",
    "file_counter = 0\n",
    "# For each roleID in the papers list (\"roles\"), check if a json file exists,\n",
    "# else: send an API request for that roleID, and make a new json file, labeled by Role ID number\n",
    "for ident in roles:\n",
    "    file = \"{}.json\"\n",
    "    \n",
    "    if os.path.exists(file.format(ident[5:])):\n",
    "        file_counter += 1\n",
    "    else:\n",
    "        AGU_AUTHORS = AGU_API + str(ident)\n",
    "        try:\n",
    "            data = requests.get(AGU_AUTHORS).json()\n",
    "            new_row = {\n",
    "            \"RoleID\":data[\"_url\"],\n",
    "            \"Role\":data[\"Role\"],\n",
    "            \"PaperID\":data[\"Parent_Entry\"],\n",
    "            \"Position\":data[\"Priority\"],\n",
    "            \"FirstName\":data[\"Person_FirstName\"],\n",
    "            \"LastName\":data[\"Person_LastName\"],\n",
    "            \"MiddleName\":data[\"Person_MiddleName\"],\n",
    "            \"Affiliation\":data[\"Person_Affiliation\"],\n",
    "            \"City\":data[\"Person_City\"],\n",
    "            \"Country\":data[\"Person_Country\"],\n",
    "            \"ORCID\":data[\"Person_ORCIDiD\"]\n",
    "            }\n",
    "            \n",
    "            with open(file.format(ident[5:]),\"w\") as outfile:\n",
    "                json.dump(new_row, outfile)\n",
    "            file_counter += 1\n",
    "        # Print error message for failed API query\n",
    "        except Exception as err:\n",
    "            print(\"Harvesting failed for {0} with the following reason: {1}\".format(ident, err))\n",
    "print('Finished role requests at',datetime.datetime.now()) # Stop clock on API queries\n",
    "print('Retrieved',file_counter,'author roles \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7237a",
   "metadata": {},
   "source": [
    "## Convert author roles - push json files to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to roles json files\n",
    "path_to_json = path + \"json_roles/\"\n",
    "json_files = [file for file in os.listdir(path_to_json) if file.endswith('.json')]\n",
    "\n",
    "# Define pandas Dataframe with the columns I want to get from the json\n",
    "role_results = pd.DataFrame(columns=['RoleID','Role','PaperID','Position','FirstName',\n",
    "                                     'LastName','MiddleName','Affiliation','City','Country','ORCID'])\n",
    "\n",
    "# Check if file already exists, else: push the json files into a new excel \"roles.xlsx\"\n",
    "if os.path.exists(path + roles_outfile):\n",
    "    print(\"Author affiliation results in\",roles_outfile)\n",
    "else:\n",
    "    # We need both the json and an index number, so use enumerate()\n",
    "    for index, js in enumerate(json_files):\n",
    "        with open(os.path.join(path_to_json, js)) as json_file:\n",
    "            json_text = json.load(json_file)\n",
    "\n",
    "            # here you need to know the layout of your json and each json has to have the same structure\n",
    "            roleID = json_text['RoleID']\n",
    "            role = json_text['Role']\n",
    "            paperID = json_text['PaperID']\n",
    "            position = json_text['Position']\n",
    "            firstname = json_text['FirstName']\n",
    "            lastname = json_text['LastName']\n",
    "            middlename = json_text['MiddleName']\n",
    "            affiliation = json_text['Affiliation']\n",
    "            city = json_text['City']\n",
    "            country = json_text['Country']\n",
    "            orcid = json_text['ORCID']\n",
    "\n",
    "            # here I push a list of data into a pandas DataFrame at row given by 'index'\n",
    "            role_results.loc[index] = [roleID, role, paperID, position, firstname, lastname, middlename, affiliation, city, country, orcid]                      \n",
    "\n",
    "# Save excel file of role results\n",
    "    role_results.to_excel(path + roles_outfile, sheet_name='role_results', index=False)\n",
    "    print(\"Saved author affiliation results to\", roles_outfile)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc088d2",
   "metadata": {},
   "source": [
    "## Merge authors & abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file already exists, else: transform the data and merge; save to new excel \"results.xlsx\"\n",
    "if os.path.exists(path + merged_outfile):\n",
    "    print(\"Final merged results in\",merged_outfile)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Read abstracts and role results from excel, and make DataFrames\n",
    "    paper_results = pd.read_excel(path + papers_outfile, sheet_name='paper_results')\n",
    "    aff_results = pd.read_excel(path + roles_outfile, sheet_name='role_results')\n",
    "    papers = pd.DataFrame(paper_results)\n",
    "    affs = pd.DataFrame(aff_results)\n",
    "\n",
    "    # Method to remove HTML tags from abstracts\n",
    "    def remove_tags(string):\n",
    "        result = re.sub('<.*?>','',string)\n",
    "        return result\n",
    "\n",
    "    # Format Abstracts\n",
    "    papers[\"Abstract\"] = (\n",
    "        papers[\"Abstract\"].astype(str)\n",
    "        .replace({'_x000D_\\n\\t_x000D_\\n_x000D_\\n':''}, regex=True)\n",
    "        .replace({'_x000D_\\n_x000D_\\n':''}, regex=True)\n",
    "        .replace({'_x000D_\\n\\t':''}, regex=True)\n",
    "        .apply(lambda cw : remove_tags(cw))\n",
    "    )\n",
    "\n",
    "    # Format Titles\n",
    "    papers[\"Title\"] = (\n",
    "        papers[\"Title\"].apply(lambda cw : remove_tags(cw))\n",
    "        .replace({'_x000D_\\n':''}, regex=True)\n",
    "    )\n",
    "\n",
    "    # Format Publications\n",
    "    papers[\"Pub\"] = pub + papers[\"FinalPaperNumber\"] + \".\"\n",
    "\n",
    "    # Format links\n",
    "    papers[\"properties.ELECTR\"] = \"https://agu.confex.com/agu/\" + meetingcode + \"/meetingapp.cgi/\" + papers[\"_url\"]\n",
    "\n",
    "    # Format Authors\n",
    "    # create a new column \"Author\" by concatenating last name, first name, and middle name (if not NA)\n",
    "    affs[\"Author\"] = affs[\"LastName\"] + \", \" + affs[\"FirstName\"]\n",
    "    affs.loc[~affs[\"MiddleName\"].isna(), \"Author\"] += \" \" + affs[\"MiddleName\"]\n",
    "\n",
    "    # Format ORCiDs\n",
    "    affs[\"ORCIDs\"] = \"<ORCID>\" + affs[\"ORCID\"] + \"</ORCID>\"\n",
    "\n",
    "    # Format full affiliations\n",
    "    affs[\"Aff_full\"] = (\n",
    "        affs[\"Affiliation\"] + \", \" + \n",
    "        affs[\"City\"] + \", \" + \n",
    "        affs[\"Country\"] + \" \" + \n",
    "        affs[\"ORCIDs\"])\n",
    "\n",
    "    # Replace missing values with affiliations without ORCID\n",
    "    affs[\"Aff_full\"] = affs[\"Aff_full\"].fillna(\n",
    "        affs[\"Affiliation\"] + \", \" + \n",
    "        affs[\"City\"] + \", \" + \n",
    "        affs[\"Country\"])\n",
    "\n",
    "    # Sort author/aff list by Paper, and then Position; Sort paper list by PaperID\n",
    "    affs = affs.sort_values(by=['PaperID', 'Position'])\n",
    "    papers = papers.sort_values(by=['_url'])\n",
    "\n",
    "    # Select needed columns as new dataframes\n",
    "    affs_data = affs[['Author','Aff_full','PaperID','Position']]\n",
    "    papers_data = papers[['Abstract','Title','Pub','_url','properties.ELECTR']]\n",
    "\n",
    "    # Aggregate Authors and their Affiliations by PaperID\n",
    "    affs_data = affs_data.replace(np.nan,'N/A')\n",
    "    affs_data = affs_data.groupby([\"PaperID\"]).agg(Authors=(\"Author\", \"; \".join), Aff_full=(\"Aff_full\", \"; \".join))\n",
    "\n",
    "    # Merge papers and authors/affils by PaperID\n",
    "    papers_data.rename(columns = {'_url':'PaperID'}, inplace = True)\n",
    "    merged = pd.merge(papers_data, affs_data, how='left', on='PaperID')\n",
    "\n",
    "    # Save merged data to new excel \"results.xlsx\"\n",
    "    merged.to_excel(path + merged_outfile, sheet_name='final_results', index=False)\n",
    "    print(\"Saved\",len(merged),\"records to\",merged_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f72431",
   "metadata": {},
   "source": [
    "## Output final data as json and tagged format records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curation note: Go into final results excel file and remove 'test abstracts', etc. Fill in missing pub info.\n",
    "# Then proceed with json and tagged process\n",
    "\n",
    "# Read curated \"results.xlsx\" file\n",
    "os.chdir(path)\n",
    "data = pd.read_excel(merged_outfile, sheet_name=0)\n",
    "dt = pd.DataFrame(data)\n",
    "\n",
    "# Grab metadata from columns and create lists\n",
    "auths_ls = [a if a != \"nan\" else \"\" for a in dt[\"Authors\"].astype(str)]\n",
    "# affs_ls = [a if a != \"nan\" else \"\" for a in dt[\"Aff_full\"].astype(str)]\n",
    "affs_ls = [a.split('; ') if a != \"nan\" else [] for a in dt[\"Aff_full\"].astype(str)]\n",
    "abs_ls = [a if a != \"nan\" else \"\" for a in dt[\"Abstract\"].astype(str)]\n",
    "titles_ls = [t if t != \"nan\" else \"\" for t in dt[\"Title\"].astype(str)]\n",
    "pubs_ls = [p if p != \"nan\" else \"\" for p in dt[\"Pub\"].astype(str)]\n",
    "ELECTR_ls = [e if e != 'nan' else \"\" for e in dt['properties.ELECTR'].astype(str)]\n",
    "links_ls = [{\"ELECTR\": e} if e else \"\" for e in ELECTR_ls]\n",
    "\n",
    "# Repackage data into list of records\n",
    "records = []\n",
    "for auths, affs, title, pub, abstract, link in zip(auths_ls, affs_ls, titles_ls, pubs_ls, abs_ls, links_ls):\n",
    "    records.append({\"authors\":auths,\n",
    "                    \"affiliations\":affs,\n",
    "                    \"pubdate\":pubdate,\n",
    "                    \"title\":title,\n",
    "                    \"publication\":pub,\n",
    "                    \"abstract\":abstract,\n",
    "                    \"properties\":link,\n",
    "                    \"source\":\"ADS\"})\n",
    "        \n",
    "# Save json file of complete records\n",
    "with open(path + json_output, 'w') as outfile:\n",
    "    json.dump(records, outfile)\n",
    "print(\"Saved\",len(records),\"records as\",json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03ae77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyingest Serializer - Transform json into tagged format\n",
    "f = open(path + json_output)\n",
    "json_file = json.load(f)\n",
    "outputfp = open(path + tagged, 'a')\n",
    "for record in json_file:\n",
    "    serializer = Tagged()\n",
    "    serializer.write(record, outputfp)\n",
    "outputfp.close()\n",
    "print(\"Saved records as\",tagged)\n",
    "\n",
    "# Curation note: remove the \"N/A\"s from the tagged file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5166e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
